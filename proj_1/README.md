# Grupo

- Henrique Coelho Beltr√£o
- Henrique Gabriel Gasparelo
- Jos√© Thevez Gomes Guedes

# Observa√ß√£o

Utilizamos a extens√£o do VSCode [Live Share](https://github.com/Microsoft/live-share) durante a escrita dos c√≥digos.

# Relat√≥rio 1: Implementar RL para Rob√¥ de reciclagem

## Introdu√ß√£o

Neste projeto foi desenvolvido um agente de aprendizado por refor√ßo para controlar a opera√ß√£o de um rob√¥ cujo objetivo √© a coleta de latas. A implementa√ß√£o segue o enunciado do Exemplo 3.3 do livro Reinforcement Learning: An Introduction (Second Edition), p√°gina 53.

O agente foi modelado com dois estados poss√≠veis: bateria alta e bateria baixa. Quando o rob√¥ est√° no estado de bateria baixa, ele pode escolher entre tr√™s a√ß√µes:

- Aguardar, recebendo uma recompensa ùëü<sub>wait</sub>.

- Buscar, com probabilidade $1-\beta$ da busca ser bem-sucedida, recebendo uma recompensa ùëü<sub>search</sub>, e com uma probabilidade Œ≤ da bateria se esgotar completamente. Situa√ß√£o em que o rob√¥ precisa ser resgatado, retornando ao estado de bateria cheia e recebendo uma penalidade de ‚àí3.

- Recarregar, retorna o estado da bateria ao n√≠vel alto.

J√° no estado de bateria alta, o agente tem a seguintes op√ß√µes de a√ß√µes:

- Aguardar, recebendo uma recompensa ùëü<sub>wait</sub>.

- Buscar, recebendo uma recompensa ùëü<sub>search</sub>, com uma probabilidade Œ± d√™ o estado da bateria ser atualizado para baixo.

## Decis√µes tomadas

O problema descrito apresenta caracter√≠sticas que tornaram necess√°rias escolhas espec√≠ficas para o funcionamento correto do algoritmo, as mudan√ßas escolhidas foram as seguintes:

- Apresenta√ß√£o da pol√≠tica: Como o problema possui diferentes estados e cada estado possui uma variedade de a√ß√µes poss√≠veis para o agente, foi preferido modelar a pol√≠tica como uma matriz, em que as linhas 1 e 2 representam, respectivamente, os valores correspondentes as a√ß√µes dos estados energia baixa e energia alta, e as colunas 1, 2 e 3 representam, respectivamente, os valores correspondentes as a√ß√µes "Buscar", "Aguardar" e "Recarregar".

- Atualiza√ß√£o da pol√≠tica: Como o problema possui v√°rios pares estado-a√ß√£o, foi necess√°rio usar o m√©todo do m√°ximo q-value para o algoritmo de Diferen√ßa Temporal, esse m√©todo consiste em atualizar o valor da pol√≠tica de um determinado par estado-a√ß√£o, usando o valor da recompensa por chegar no pr√≥ximo no estado, o valor atual desse par na matriz, e o m√°ximo dos valores dos pares estado-a√ß√£o correspondentes ao pr√≥ximo estado obtido ap√≥s a a√ß√£o, mais precisamente, seja $M$ a matriz descrita no t√≥pico anterior, $i$ o √≠ndice do estado antes da a√ß√£o, $i'$ o √≠ndice do estado ap√≥s a a√ß√£o, $j$ o √≠ndice da a√ß√£o, $r$ a recompensa obtida e $\gamma$ a taxa de aprendizado, ent√£o, a forma que o m√©todo atualiza a pol√≠tica √© a seguinte:

$$
M_{i j} = M_{i j} + \gamma (r + max{M_{i'} - M_{i j}})
$$

- Atualiza√ß√µes por epoch: Em cada epoch, foi preferido atualizar a pol√≠tica uma √∫nica vez no fim da epoch, ao contr√°rio de atualizar a cada passo da epoch.

## C√≥digo

O c√≥digo foi implementado usando pair programming (no caso desse projeto, foi um trabalho simult√¢neo de 3 pessoas), onde foram escritos os seguintes arquivos:

### main.py
