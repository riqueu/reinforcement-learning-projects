# Grupo

- Henrique Coelho Beltr√£o
- Henrique Gabriel Gasparelo
- Jos√© Thevez Gomes Guedes

# Observa√ß√£o

Utilizamos a extens√£o do VSCode [Live Share](https://github.com/Microsoft/live-share) durante a escrita dos c√≥digos.

# Relat√≥rio 1: Implementar RL para Rob√¥ de reciclagem

## Introdu√ß√£o

Neste projeto foi desenvolvido um agente de aprendizado por refor√ßo para controlar a opera√ß√£o de um rob√¥ cujo objetivo √© a coleta de latas. A implementa√ß√£o segue o enunciado do Exemplo 3.3 do livro Reinforcement Learning: An Introduction (Second Edition), p√°gina 53.

O agente foi modelado com dois estados poss√≠veis: bateria alta e bateria baixa. Quando o rob√¥ est√° no estado de bateria baixa, ele pode escolher entre tr√™s a√ß√µes:

- Aguardar, recebendo uma recompensa ùëü<sub>wait</sub>.

- Buscar, com probabilidade $1-\beta$ da busca ser bem-sucedida, recebendo uma recompensa ùëü<sub>search</sub>, e com uma probabilidade Œ≤ da bateria se esgotar completamente. Situa√ß√£o em que o rob√¥ precisa ser resgatado, retornando ao estado de bateria cheia e recebendo uma penalidade de ‚àí3.

- Recarregar, retorna o estado da bateria ao n√≠vel alto.

J√° no estado de bateria alta, o agente tem a seguintes op√ß√µes de a√ß√µes:

- Aguardar, recebendo uma recompensa ùëü<sub>wait</sub>.

- Buscar, recebendo uma recompensa ùëü<sub>search</sub>, com uma probabilidade Œ± d√™ o estado da bateria ser atualizado para baixo.

## Decis√µes tomadas

O problema descrito apresenta caracter√≠sticas que tornaram necess√°rias escolhas espec√≠ficas para o funcionamento correto do algoritmo, as mudan√ßas escolhidas foram as seguintes:

- Apresenta√ß√£o da pol√≠tica: Como o problema possui diferentes estados e cada estado possui uma variedade de a√ß√µes poss√≠veis para o agente, foi preferido modelar a pol√≠tica como uma matriz, em que as linhas 1 e 2 representam, respectivamente, os valores correspondentes as a√ß√µes dos estados energia baixa e energia alta, e as colunas 1, 2 e 3 representam, respectivamente, os valores correspondentes as a√ß√µes "Buscar", "Aguardar" e "Recarregar".

- Atualiza√ß√£o da pol√≠tica: Como o problema possui v√°rios pares estado-a√ß√£o, foi necess√°rio usar o m√©todo do m√°ximo q-value para o algoritmo de Diferen√ßa Temporal, esse m√©todo consiste em atualizar o valor da pol√≠tica de um determinado par estado-a√ß√£o, usando o valor da recompensa por chegar no pr√≥ximo no estado, o valor atual desse par na matriz, e o m√°ximo dos valores dos pares estado-a√ß√£o correspondentes ao pr√≥ximo estado obtido ap√≥s a a√ß√£o, mais precisamente, seja $M$ a matriz descrita no t√≥pico anterior, $i$ o √≠ndice do estado antes da a√ß√£o, $i'$ o √≠ndice do estado ap√≥s a a√ß√£o, $j$ o √≠ndice da a√ß√£o, $r$ a recompensa obtida e $\gamma$ a taxa de aprendizado, ent√£o, a forma que o m√©todo atualiza a pol√≠tica √© a seguinte:

$$
M_{i j} = M_{i j} + \gamma (r + max{M_{i'} - M_{i j}})
$$

- Atualiza√ß√µes por epoch: Em cada epoch, foi preferido atualizar a pol√≠tica uma √∫nica vez no fim da epoch, ao contr√°rio de atualizar a cada passo da epoch.

## C√≥digo

O c√≥digo foi implementado usando pair programming (no caso desse projeto, foi um trabalho simult√¢neo de 3 pessoas), onde foram escritos os seguintes arquivos:

### main.py

## Resultados
Como resultados, obtive-se o gr√°fico de recompensas totais por epoch e a pol√≠tica √≥tima aprendida, ambos plotados usando o arquivo ‚Äúviz.py‚Äù. Segue abaixo o gr√°fico da m√©dia das recompensas por epoch:

<div style="text-align: center;">
  <img src="rewards_multiple_runs.png" width="600"/>
</div> 

Esse gr√°fico, como j√° descrito, cont√©m a m√©dia das recompensas totais obtidas em cada epoch, que como pode ser observado, mostra um aprendizado  acentuado nas epochs iniciais e uma estagna√ß√£o ap√≥s, o que pode estar refletindo que o agente...
O que tamb√©m √© evidenciado ao analisar a pol√≠tica √≥tima aprendida, sendo plotada no heatmap a seguir:

<div style="text-align: center;">
  <img src="optimal_policy_heatmap.png" width="600"/>
</div> 

√â poss√≠vel notar que o agente aprendeu a sempre que estiver com bateria alta buscar, e quando a bateria estiver baixa recarregar. Mostrando uma abordagem de menos risco para obter recompensas. Um ponto tamb√©m interessante √© a prefer√™ncia por n√£o usar a a√ß√£o de esperar, refletindo a baixa recompensa de esperar, sendo ent√£o prefer√≠vel mesmo no estado de baixa bateria recarregar ao env√©s de esperar, uma vez que buscando com a bateria alta a recompensa ser√° mais alta e sem risco de receber puni√ß√µes. 

Esta preferencia pode ser observada no gr√°fico de barras a seguir, que evidencia a quantidade de vezes que cada a√ß√£o foi tomada:

<div style="text-align: center;">
  <img src="action_distribution.png" width="600"/>
</div> 

O que reafirma o que foi constatado na pol√≠tica apreendida, √© observado em sua maioria a a√ß√£o de ‚Äúbuscar‚Äù, seguida pela ‚Äúrecarregar‚Äù. Com uma √≠nfima por√ß√£o da a√ß√£o ‚Äúesperar‚Äù, que provavelmente √© realizada no in√≠cio antes do aprendizado da pol√≠tica √≥tima e tamb√©m por conta da metodologia explorat√≥ria ‚Äúepsilon-greedy‚Äù, que com uma probabilidade Œµ escolhe uma a√ß√£o aleat√≥ria dentre as dispon√≠veis para estado do agente.
