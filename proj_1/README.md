# Grupo

- Henrique Coelho Beltr√£o
- Henrique Gabriel Gasparelo
- Jos√© Thevez Gomes Guedes

# Observa√ß√£o

Utilizamos a extens√£o do VSCode [Live Share](https://github.com/Microsoft/live-share) durante a escrita dos c√≥digos.

# Relat√≥rio 1: Implementar RL para Rob√¥ de reciclagem

## Introdu√ß√£o

Neste projeto foi desenvolvido um agente de aprendizado por refor√ßo para controlar a opera√ß√£o de um rob√¥ cujo objetivo √© a coleta de latas. A implementa√ß√£o segue o enunciado do Exemplo 3.3 do livro Reinforcement Learning: An Introduction (Second Edition), p√°gina 52.

O agente foi modelado com dois estados poss√≠veis: bateria alta e bateria baixa. Quando o rob√¥ est√° no estado de bateria baixa, ele pode escolher entre tr√™s a√ß√µes:

- Aguardar, recebendo uma recompensa ùëü<sub>wait</sub>.

- Buscar, com probabilidade $\beta$ da busca ser bem-sucedida, recebendo uma recompensa ùëü<sub>search</sub>, e com uma probabilidade $1-\beta$ da bateria se esgotar completamente. Situa√ß√£o em que o rob√¥ precisa ser resgatado, retornando ao estado de bateria cheia e recebendo uma penalidade de ‚àí3.

- Recarregar, retorna o estado da bateria ao n√≠vel alto.

J√° no estado de bateria alta, o agente tem a seguintes op√ß√µes de a√ß√µes:

- Aguardar, recebendo uma recompensa ùëü<sub>wait</sub>.

- Buscar, recebendo uma recompensa ùëü<sub>search</sub>, com uma probabilidade $1-\alpha$ d√™ o estado da bateria ser atualizado para baixo.

## Decis√µes tomadas

O problema descrito apresenta caracter√≠sticas que tornaram necess√°rias escolhas espec√≠ficas para o funcionamento correto do algoritmo, as mudan√ßas escolhidas foram as seguintes:

- Apresenta√ß√£o da pol√≠tica: Como o problema possui diferentes estados e cada estado possui uma variedade de a√ß√µes poss√≠veis para o agente, foi preferido modelar a pol√≠tica como uma matriz, em que as linhas 1 e 2 representam, respectivamente, os valores correspondentes as a√ß√µes dos estados energia baixa e energia alta, e as colunas 1, 2 e 3 representam, respectivamente, os valores correspondentes as a√ß√µes "Buscar", "Aguardar" e "Recarregar".

- Atualiza√ß√£o da pol√≠tica: Como o problema possui v√°rios pares estado-a√ß√£o, foi necess√°rio usar o m√©todo do m√°ximo q-value para o algoritmo de Diferen√ßa Temporal, esse m√©todo consiste em atualizar o valor da pol√≠tica de um determinado par estado-a√ß√£o, usando o valor da recompensa por chegar no pr√≥ximo no estado, o valor atual desse par na matriz, e o m√°ximo dos valores dos pares estado-a√ß√£o correspondentes ao pr√≥ximo estado obtido ap√≥s a a√ß√£o, mais precisamente, seja $M$ a matriz descrita no t√≥pico anterior, $i$ o √≠ndice do estado antes da a√ß√£o, $i'$ o √≠ndice do estado ap√≥s a a√ß√£o, $j$ o √≠ndice da a√ß√£o, $r$ a recompensa obtida e $\gamma$ a taxa de aprendizado, ent√£o, a forma que o m√©todo atualiza a pol√≠tica √© a seguinte:

$$
M_{i j} = M_{i j} + \gamma (r + max{M_{i'} - M_{i j}})
$$

- Atualiza√ß√µes por epoch: Em cada epoch, foi preferido atualizar a pol√≠tica a cada 200 passos, ao contr√°rio de atualizar a cada passo da epoch.
  
- Hiperpar√¢metros: Para a atualiza√ß√£o da pol√≠tica foi utilizado o learning rate de 0.1 e para epsilon-greedy foi utilizado o Œµ de 0.001.

## C√≥digo

O c√≥digo foi implementado usando pair programming (no caso desse projeto, foi um trabalho simult√¢neo de 3 pessoas), onde foram escritos os seguintes arquivos:

### [main.py](main.py)

O arquivo main.py cont√©m o loop central da simula√ß√£o e do treinamento. Nele s√£o definidos os par√¢metros: n√∫mero de epochs (1000) e passos por epoch (1000). Tamb√©m s√£o configurados os valores das probabilidades Œ± = 0.3 e Œ≤ = 0.2. Para as recompensas, foram atribu√≠dos 3.5 para ùëü<sub>search</sub>, e de 0.5 para o ùëü<sub>wait</sub>. Esses valores foram escolhidos para representar um cen√°rio no qual a busca √© relativamente segura e altamente recompensat√≥ria.

### [utils.py](utils.py)

No arquivo utils.py, est√° implementada a l√≥gica de funcionamento do rob√¥, estruturada em tr√™s classes:

- **State**: respons√°vel por armazenar os estados poss√≠veis do rob√¥, sendo dois: bateria baixa e bateria alta;

- **Environment**: utiliza os par√¢metros (probabilidades e recompensas) definidos em main.py para executar as transi√ß√µes de estados e calcular as recompensas;

- **Robot**: representa o agente, respons√°vel pela tomada de decis√£o em cada estado com base em uma pol√≠tica de aprendizado por refor√ßo. Al√©m disso, realiza a atualiza√ß√£o da pol√≠tica ao longo do treinamento.

### [viz.py](viz.py)

O arquivo viz.py concentra todas as fun√ß√µes respons√°veis pela visualiza√ß√£o dos resultados obtidos durante o treinamento do rob√¥. Utilizando das bibliotecas Matplotlib, Seaborn e NumPy para gerar e salvar os gr√°ficos.

Al√©m da cria√ß√£o dos gr√°ficos, o m√≥dulo tamb√©m realiza o tratamento dos valores resultantes da simula√ß√£o implementando a fun√ß√£o softmax, para converter os valores da pol√≠tica aprendida em probabilidades, facilitando a interpreta√ß√£o da estrat√©gia adotada pelo agente.

## Resultados
Como resultados, obteve-se o gr√°fico de recompensas totais por epoch e a pol√≠tica √≥tima aprendida. Segue abaixo o gr√°fico das recompensas por epoch de 10 experimentos realizados e, destacado em vermelho, a m√©dia das recompensas:

<div style="text-align: center;">
  <img src="rewards_multiple_runs.png" width="600"/>
</div> 

Esse gr√°fico, como j√° descrito, cont√©m a m√©dia das recompensas totais obtidas em cada epoch, que como pode ser observado, mostra um aprendizado acentuado nas epochs iniciais e uma estagna√ß√£o ap√≥s isso. O que pode estar refletindo que o agente aprendeu uma pol√≠tica √≥tima de forma r√°pida e que ela n√£o sofreu grandes altera√ß√µes ap√≥s as epochs iniciais. O que tamb√©m √© evidenciado ao analisar a pol√≠tica √≥tima aprendida, sendo plotada no heatmap a seguir:

<div style="text-align: center;">
  <img src="optimal_policy_heatmap.png" width="600"/>
</div> 

√â poss√≠vel notar que o agente aprendeu a usar a a√ß√£o "Buscar" sempre que estiver com bateria alta, e "Recarregar" quando a bateria estiver baixa. Mostrando uma abordagem de menos risco para obter recompensas. Um ponto tamb√©m interessante √© a prefer√™ncia por n√£o usar a a√ß√£o de aguardar, refletindo a baixa recompensa desta a√ß√£o. Sendo ent√£o prefer√≠vel mesmo no estado de baixa bateria, recarregar ao inv√©s de aguardar, uma vez que buscando com a bateria alta a recompensa ser√° maior e sem risco de receber puni√ß√µes. 

Esta prefer√™ncia pode ser observada no gr√°fico de barras a seguir, que evidencia a quantidade de vezes que cada a√ß√£o foi tomada:

<div style="text-align: center;">
  <img src="action_distribution.png" width="600"/>
</div> 

Reafirmando o constatado na pol√≠tica apreendida, √© observado em sua maioria a a√ß√£o de ‚Äúbuscar‚Äù, seguida pela ‚Äúrecarregar‚Äù. Tamb√©m √© evidente uma √≠nfima por√ß√£o da a√ß√£o ‚Äúaguardar‚Äù, o que provavelmente se d√° pelas escolhas tomadas no in√≠cio, antes da aprendizagem da pol√≠tica, e tamb√©m por conta da metodologia explorat√≥ria ‚Äúepsilon-greedy‚Äù, que com uma probabilidade Œµ (0.001) escolhe uma a√ß√£o aleat√≥ria dentre as dispon√≠veis para o estado do agente.

## Conclus√£o

Portanto, conclui-se que o agente, neste projeto, o rob√¥ reciclador, conseguiu aprender uma estrat√©gia capaz de aumentar seu ganho de recompensa. Vale ressaltar que determinados testes com diferentes par√¢metros foram capazes de gerar diferentes resultados. Por exemplo, diminuir a diferen√ßa entre ùëü<sub>wait</sub> e ùëü<sub>search</sub>, enquanto se aumenta as probabilidades de diminui√ß√£o de bateria, geram uma pol√≠tica baseada em aguardar com poucas ocorr√™ncias de a√ß√µes como ‚Äúbuscar‚Äù, uma vez que a diferen√ßa de recompensa √© baixa e os riscos s√£o menores. Deste modo, o c√≥digo implementado soluciona de forma coerente o problema, e se mostra livre para conjuntos de par√¢metros distintos dos usados para a gera√ß√£o dos resultados, permitindo diferentes an√°lises.
